{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[연습]_colab_bert_finetuning_SKTstyle.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b6e854515fe84a2ba0fa9e90e4861a3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1fedb5ef7c6d46c8aace89ee3693e9be","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b7807ed31aa54fd49fb7e51af75cf8c4","IPY_MODEL_0c4ab22c2a544303a419f7b47a9d82e0"]}},"1fedb5ef7c6d46c8aace89ee3693e9be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7807ed31aa54fd49fb7e51af75cf8c4":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_be597f2fc357461588cba80985b72ee4","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"danger","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc79126fd444415197f4326ed04c593b"}},"0c4ab22c2a544303a419f7b47a9d82e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_40cd1a185f2e43bd832a8bb7f8aca199","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  0% 0/10 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_52dcb9b6c83a4668934f3dbc44e285e6"}},"be597f2fc357461588cba80985b72ee4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cc79126fd444415197f4326ed04c593b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40cd1a185f2e43bd832a8bb7f8aca199":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"52dcb9b6c83a4668934f3dbc44e285e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"N8ebTsFBQRHf","colab_type":"code","colab":{}},"source":["# [연습]_colab_bert_finetuning.ipynb\n","# 200102(목) 12:17~\n","# 천성욱"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpRdsFF4QR_R","colab_type":"code","outputId":"416aa0aa-2e95-4703-ebcc-058b0379e040","executionInfo":{"status":"ok","timestamp":1578138752707,"user_tz":-540,"elapsed":2305,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DwEVnjmwQDCV","colab_type":"code","outputId":"7eb693b0-0d7a-4e9e-ce06-dc7c74e5d55f","executionInfo":{"status":"ok","timestamp":1578138854818,"user_tz":-540,"elapsed":80332,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install mxnet-cu101\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece==0.1.85\n","!pip install transformers==2.1.1\n","!pip install torch==1.3.1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting mxnet-cu101\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/aa/474e97edaa2064c4e57dfb5685290ffc188592878c8bc58a1dca770872b5/mxnet_cu101-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (551.3MB)\n","\u001b[K     |████████████████████████████████| 551.3MB 26kB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.17.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.21.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n","Installing collected packages: graphviz, mxnet-cu101\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-cu101-1.5.1.post0\n","Collecting gluonnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/42/85f6cf7e13e222b2dc552059de8d37fe5956b9fab37f95dfacfa4e15a124/gluonnlp-0.8.2.tar.gz (237kB)\n","\u001b[K     |████████████████████████████████| 245kB 45.2MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.17.4)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.8.2-cp36-none-any.whl size=293515 sha256=516a6a44d5a0d4181d52efc973aa151e545178440c5501995ac09408bcb84caf\n","  Stored in directory: /root/.cache/pip/wheels/a4/aa/61/0aebc5c078c4b1ccf325cd7579932b99403008da6e7ce6b68f\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.8.2\n","Requirement already satisfied: sentencepiece==0.1.85 in /usr/local/lib/python3.6/dist-packages (0.1.85)\n","Collecting transformers==2.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n","\u001b[K     |████████████████████████████████| 317kB 13.0MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.10.40)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.85)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.17.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.28.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.0.35)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.12.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.21.0)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (1.13.40)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.2.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.12.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2019.11.28)\n","Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers==2.1.1) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers==2.1.1) (0.15.2)\n","Installing collected packages: transformers\n","  Found existing installation: transformers 2.2.0\n","    Uninstalling transformers-2.2.0:\n","      Successfully uninstalled transformers-2.2.0\n","Successfully installed transformers-2.1.1\n","Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VzVXdBD7P_mV","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","# from transformers import WarmupLinearSchedule as get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVtk3tCtQI27","colab_type":"code","colab":{}},"source":["tor_odic_weights = torch.load('/content/drive/My Drive/금융문자/모델링/model_KorBERT(ETRI)/1_bert_download_001_bert_morp_pytorch/001_bert_morp_pytorch/pytorch_model.bin')\n","dic_weights = dict(tor_odic_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h521aggnQOig","colab_type":"code","outputId":"c192076e-2d4d-4319-d446-9d141eb8a332","executionInfo":{"status":"ok","timestamp":1578137198186,"user_tz":-540,"elapsed":1578,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["d_weight_new = dict()\n","int_lcnt_f = len(list(dic_weights.items())) -8    # BERT 모델 부분만 로드하기 위해서 8을 빼야한다.\n","for idx, (k, v) in enumerate(dic_weights.items()):\n","    if idx ==  int_lcnt_f:\n","        break\n","    print(k[5:])\n","    d_weight_new[k[5:]] = v"],"execution_count":4,"outputs":[{"output_type":"stream","text":["embeddings.word_embeddings.weight\n","embeddings.position_embeddings.weight\n","embeddings.token_type_embeddings.weight\n","embeddings.LayerNorm.weight\n","embeddings.LayerNorm.bias\n","encoder.layer.0.attention.self.query.weight\n","encoder.layer.0.attention.self.query.bias\n","encoder.layer.0.attention.self.key.weight\n","encoder.layer.0.attention.self.key.bias\n","encoder.layer.0.attention.self.value.weight\n","encoder.layer.0.attention.self.value.bias\n","encoder.layer.0.attention.output.dense.weight\n","encoder.layer.0.attention.output.dense.bias\n","encoder.layer.0.attention.output.LayerNorm.weight\n","encoder.layer.0.attention.output.LayerNorm.bias\n","encoder.layer.0.intermediate.dense.weight\n","encoder.layer.0.intermediate.dense.bias\n","encoder.layer.0.output.dense.weight\n","encoder.layer.0.output.dense.bias\n","encoder.layer.0.output.LayerNorm.weight\n","encoder.layer.0.output.LayerNorm.bias\n","encoder.layer.1.attention.self.query.weight\n","encoder.layer.1.attention.self.query.bias\n","encoder.layer.1.attention.self.key.weight\n","encoder.layer.1.attention.self.key.bias\n","encoder.layer.1.attention.self.value.weight\n","encoder.layer.1.attention.self.value.bias\n","encoder.layer.1.attention.output.dense.weight\n","encoder.layer.1.attention.output.dense.bias\n","encoder.layer.1.attention.output.LayerNorm.weight\n","encoder.layer.1.attention.output.LayerNorm.bias\n","encoder.layer.1.intermediate.dense.weight\n","encoder.layer.1.intermediate.dense.bias\n","encoder.layer.1.output.dense.weight\n","encoder.layer.1.output.dense.bias\n","encoder.layer.1.output.LayerNorm.weight\n","encoder.layer.1.output.LayerNorm.bias\n","encoder.layer.2.attention.self.query.weight\n","encoder.layer.2.attention.self.query.bias\n","encoder.layer.2.attention.self.key.weight\n","encoder.layer.2.attention.self.key.bias\n","encoder.layer.2.attention.self.value.weight\n","encoder.layer.2.attention.self.value.bias\n","encoder.layer.2.attention.output.dense.weight\n","encoder.layer.2.attention.output.dense.bias\n","encoder.layer.2.attention.output.LayerNorm.weight\n","encoder.layer.2.attention.output.LayerNorm.bias\n","encoder.layer.2.intermediate.dense.weight\n","encoder.layer.2.intermediate.dense.bias\n","encoder.layer.2.output.dense.weight\n","encoder.layer.2.output.dense.bias\n","encoder.layer.2.output.LayerNorm.weight\n","encoder.layer.2.output.LayerNorm.bias\n","encoder.layer.3.attention.self.query.weight\n","encoder.layer.3.attention.self.query.bias\n","encoder.layer.3.attention.self.key.weight\n","encoder.layer.3.attention.self.key.bias\n","encoder.layer.3.attention.self.value.weight\n","encoder.layer.3.attention.self.value.bias\n","encoder.layer.3.attention.output.dense.weight\n","encoder.layer.3.attention.output.dense.bias\n","encoder.layer.3.attention.output.LayerNorm.weight\n","encoder.layer.3.attention.output.LayerNorm.bias\n","encoder.layer.3.intermediate.dense.weight\n","encoder.layer.3.intermediate.dense.bias\n","encoder.layer.3.output.dense.weight\n","encoder.layer.3.output.dense.bias\n","encoder.layer.3.output.LayerNorm.weight\n","encoder.layer.3.output.LayerNorm.bias\n","encoder.layer.4.attention.self.query.weight\n","encoder.layer.4.attention.self.query.bias\n","encoder.layer.4.attention.self.key.weight\n","encoder.layer.4.attention.self.key.bias\n","encoder.layer.4.attention.self.value.weight\n","encoder.layer.4.attention.self.value.bias\n","encoder.layer.4.attention.output.dense.weight\n","encoder.layer.4.attention.output.dense.bias\n","encoder.layer.4.attention.output.LayerNorm.weight\n","encoder.layer.4.attention.output.LayerNorm.bias\n","encoder.layer.4.intermediate.dense.weight\n","encoder.layer.4.intermediate.dense.bias\n","encoder.layer.4.output.dense.weight\n","encoder.layer.4.output.dense.bias\n","encoder.layer.4.output.LayerNorm.weight\n","encoder.layer.4.output.LayerNorm.bias\n","encoder.layer.5.attention.self.query.weight\n","encoder.layer.5.attention.self.query.bias\n","encoder.layer.5.attention.self.key.weight\n","encoder.layer.5.attention.self.key.bias\n","encoder.layer.5.attention.self.value.weight\n","encoder.layer.5.attention.self.value.bias\n","encoder.layer.5.attention.output.dense.weight\n","encoder.layer.5.attention.output.dense.bias\n","encoder.layer.5.attention.output.LayerNorm.weight\n","encoder.layer.5.attention.output.LayerNorm.bias\n","encoder.layer.5.intermediate.dense.weight\n","encoder.layer.5.intermediate.dense.bias\n","encoder.layer.5.output.dense.weight\n","encoder.layer.5.output.dense.bias\n","encoder.layer.5.output.LayerNorm.weight\n","encoder.layer.5.output.LayerNorm.bias\n","encoder.layer.6.attention.self.query.weight\n","encoder.layer.6.attention.self.query.bias\n","encoder.layer.6.attention.self.key.weight\n","encoder.layer.6.attention.self.key.bias\n","encoder.layer.6.attention.self.value.weight\n","encoder.layer.6.attention.self.value.bias\n","encoder.layer.6.attention.output.dense.weight\n","encoder.layer.6.attention.output.dense.bias\n","encoder.layer.6.attention.output.LayerNorm.weight\n","encoder.layer.6.attention.output.LayerNorm.bias\n","encoder.layer.6.intermediate.dense.weight\n","encoder.layer.6.intermediate.dense.bias\n","encoder.layer.6.output.dense.weight\n","encoder.layer.6.output.dense.bias\n","encoder.layer.6.output.LayerNorm.weight\n","encoder.layer.6.output.LayerNorm.bias\n","encoder.layer.7.attention.self.query.weight\n","encoder.layer.7.attention.self.query.bias\n","encoder.layer.7.attention.self.key.weight\n","encoder.layer.7.attention.self.key.bias\n","encoder.layer.7.attention.self.value.weight\n","encoder.layer.7.attention.self.value.bias\n","encoder.layer.7.attention.output.dense.weight\n","encoder.layer.7.attention.output.dense.bias\n","encoder.layer.7.attention.output.LayerNorm.weight\n","encoder.layer.7.attention.output.LayerNorm.bias\n","encoder.layer.7.intermediate.dense.weight\n","encoder.layer.7.intermediate.dense.bias\n","encoder.layer.7.output.dense.weight\n","encoder.layer.7.output.dense.bias\n","encoder.layer.7.output.LayerNorm.weight\n","encoder.layer.7.output.LayerNorm.bias\n","encoder.layer.8.attention.self.query.weight\n","encoder.layer.8.attention.self.query.bias\n","encoder.layer.8.attention.self.key.weight\n","encoder.layer.8.attention.self.key.bias\n","encoder.layer.8.attention.self.value.weight\n","encoder.layer.8.attention.self.value.bias\n","encoder.layer.8.attention.output.dense.weight\n","encoder.layer.8.attention.output.dense.bias\n","encoder.layer.8.attention.output.LayerNorm.weight\n","encoder.layer.8.attention.output.LayerNorm.bias\n","encoder.layer.8.intermediate.dense.weight\n","encoder.layer.8.intermediate.dense.bias\n","encoder.layer.8.output.dense.weight\n","encoder.layer.8.output.dense.bias\n","encoder.layer.8.output.LayerNorm.weight\n","encoder.layer.8.output.LayerNorm.bias\n","encoder.layer.9.attention.self.query.weight\n","encoder.layer.9.attention.self.query.bias\n","encoder.layer.9.attention.self.key.weight\n","encoder.layer.9.attention.self.key.bias\n","encoder.layer.9.attention.self.value.weight\n","encoder.layer.9.attention.self.value.bias\n","encoder.layer.9.attention.output.dense.weight\n","encoder.layer.9.attention.output.dense.bias\n","encoder.layer.9.attention.output.LayerNorm.weight\n","encoder.layer.9.attention.output.LayerNorm.bias\n","encoder.layer.9.intermediate.dense.weight\n","encoder.layer.9.intermediate.dense.bias\n","encoder.layer.9.output.dense.weight\n","encoder.layer.9.output.dense.bias\n","encoder.layer.9.output.LayerNorm.weight\n","encoder.layer.9.output.LayerNorm.bias\n","encoder.layer.10.attention.self.query.weight\n","encoder.layer.10.attention.self.query.bias\n","encoder.layer.10.attention.self.key.weight\n","encoder.layer.10.attention.self.key.bias\n","encoder.layer.10.attention.self.value.weight\n","encoder.layer.10.attention.self.value.bias\n","encoder.layer.10.attention.output.dense.weight\n","encoder.layer.10.attention.output.dense.bias\n","encoder.layer.10.attention.output.LayerNorm.weight\n","encoder.layer.10.attention.output.LayerNorm.bias\n","encoder.layer.10.intermediate.dense.weight\n","encoder.layer.10.intermediate.dense.bias\n","encoder.layer.10.output.dense.weight\n","encoder.layer.10.output.dense.bias\n","encoder.layer.10.output.LayerNorm.weight\n","encoder.layer.10.output.LayerNorm.bias\n","encoder.layer.11.attention.self.query.weight\n","encoder.layer.11.attention.self.query.bias\n","encoder.layer.11.attention.self.key.weight\n","encoder.layer.11.attention.self.key.bias\n","encoder.layer.11.attention.self.value.weight\n","encoder.layer.11.attention.self.value.bias\n","encoder.layer.11.attention.output.dense.weight\n","encoder.layer.11.attention.output.dense.bias\n","encoder.layer.11.attention.output.LayerNorm.weight\n","encoder.layer.11.attention.output.LayerNorm.bias\n","encoder.layer.11.intermediate.dense.weight\n","encoder.layer.11.intermediate.dense.bias\n","encoder.layer.11.output.dense.weight\n","encoder.layer.11.output.dense.bias\n","encoder.layer.11.output.LayerNorm.weight\n","encoder.layer.11.output.LayerNorm.bias\n","pooler.dense.weight\n","pooler.dense.bias\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2xTY0jXFvZLl","colab_type":"code","outputId":"b5f4e963-e909-43c5-8c13-47215596c064","executionInfo":{"status":"ok","timestamp":1578137200603,"user_tz":-540,"elapsed":587,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":150}},"source":["from collections import OrderedDict\n","odic_weight_new = OrderedDict(d_weight_new)\n","print(list(tor_odic_weights.items())[0][1].shape)\n","print(list(tor_odic_weights.items())[0])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["torch.Size([30349, 768])\n","('bert.embeddings.word_embeddings.weight', tensor([[-0.0276,  0.0102, -0.0276,  ...,  0.0029, -0.0093, -0.0042],\n","        [-0.0470,  0.0217, -0.0154,  ..., -0.0391,  0.0253,  0.0215],\n","        [-0.0147, -0.0300, -0.0310,  ..., -0.0107, -0.0058,  0.0222],\n","        ...,\n","        [-0.0589, -0.0076,  0.0200,  ...,  0.0097, -0.0124,  0.0145],\n","        [-0.0356, -0.0602, -0.0140,  ...,  0.0230, -0.0017, -0.0488],\n","        [-0.0531, -0.0201, -0.0196,  ..., -0.0237, -0.0130, -0.0030]]))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lu0ABVROREeK","colab_type":"code","outputId":"b12c0a4a-0f5c-4ec1-f0de-807934ddd9b5","executionInfo":{"status":"ok","timestamp":1578137205974,"user_tz":-540,"elapsed":3287,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from transformers import BertModel\n","from transformers import BertConfig\n","model_bert = BertModel(BertConfig(vocab_size_or_config_json_file=30349))\n","model_bert"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30349, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"oH-RVXNqT8T4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":301},"outputId":"d8536702-58bb-4dae-b7a5-305bddf90ddb","executionInfo":{"status":"ok","timestamp":1578137283057,"user_tz":-540,"elapsed":5475,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["!nvidia-smi"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Sat Jan  4 11:28:00 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o7YSIh_DTuv5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"34dbc543-a14c-420d-d0d2-d9a67917b9b4","executionInfo":{"status":"ok","timestamp":1578137380942,"user_tz":-540,"elapsed":84248,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["from transformers import BertForSequenceClassification\n","# 분류를 위한 BERT 모델 생성\n","# GPU Mem 누적 1055Mb 차지\n","# colab 1447Mb 차지\n","import time\n","time_i = time.time()\n","model_sc = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n","model_sc.cuda()\n","print(time.time() - time_i)\n","#1055Mb\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["100%|██████████| 521/521 [00:00<00:00, 171849.04B/s]\n","100%|██████████| 714314041/714314041 [01:09<00:00, 10294540.34B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["83.29525232315063\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VSDCbmC_Wee_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":278},"outputId":"d305497e-8cda-4a10-ab5e-40fd5303b139","executionInfo":{"status":"error","timestamp":1578137957476,"user_tz":-540,"elapsed":1700,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["torch.save(model_sc, "],"execution_count":24,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-5e33ac295b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'weights'"]}]},{"cell_type":"code","metadata":{"id":"J1gLdxXLVVH_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"outputId":"3327e13d-bd92-49cb-d979-396f7965f148","executionInfo":{"status":"error","timestamp":1578137807249,"user_tz":-540,"elapsed":2065,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["model_sc_01 = BertForSequenceClassification\n","model_sc_01.set_input_embeddings(model_bert, )"],"execution_count":23,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-fd28c69e7837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_sc_01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_sc_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: set_input_embeddings() missing 1 required positional argument: 'value'"]}]},{"cell_type":"code","metadata":{"id":"kAfj8yAGVQMD","colab_type":"code","colab":{}},"source":["dir(BertForSequenceClassification)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wytjEntqUDvT","colab_type":"code","colab":{}},"source":["dir(model_sc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_LBzIkOU3Wy","colab_type":"code","colab":{}},"source":["model_sc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fk0o3oWJWyo6","colab_type":"code","colab":{}},"source":["display(list(dic_weights.keys()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_kNTxq2XLFZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":150},"outputId":"e1e87458-6ceb-493a-c9fb-adf22e9ed153","executionInfo":{"status":"ok","timestamp":1578138194029,"user_tz":-540,"elapsed":1682,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["print(dic_weights['cls.predictions.bias'].shape)\n","print(dic_weights['cls.predictions.transform.dense.weight'].shape)\n","print(dic_weights['cls.predictions.transform.dense.bias'].shape)\n","print(dic_weights['cls.predictions.transform.LayerNorm.weight'].shape)\n","print(dic_weights['cls.predictions.transform.LayerNorm.bias'].shape)\n","print(dic_weights['cls.predictions.decoder.weight'].shape)\n","print(dic_weights['cls.seq_relationship.weight'].shape)\n","print(dic_weights['cls.seq_relationship.bias'].shape)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["torch.Size([30349])\n","torch.Size([768, 768])\n","torch.Size([768])\n","torch.Size([768])\n","torch.Size([768])\n","torch.Size([30349, 768])\n","torch.Size([2, 768])\n","torch.Size([2])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dn5TkRQpXnod","colab_type":"code","colab":{}},"source":["model_sc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Gd_P7a4UgLV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":349},"outputId":"dbf4a1f9-2cc4-4189-89e3-cc8c4c3b8a7a","executionInfo":{"status":"error","timestamp":1578137576295,"user_tz":-540,"elapsed":1672,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["model_sc.load_state_dict(dic_weights)#{'hi':0})#dic_weights)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-f3700dd98805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#{'hi':0})#dic_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"classifier.weight\", \"classifier.bias\". \n\tUnexpected key(s) in state_dict: \"cls.predictions.bias\", \"cls.predictions.transform.dense.weight\", \"cls.predictions.transform.dense.bias\", \"cls.predictions.transform.LayerNorm.weight\", \"cls.predictions.transform.LayerNorm.bias\", \"cls.predictions.decoder.weight\", \"cls.seq_relationship.weight\", \"cls.seq_relationship.bias\". \n\tsize mismatch for bert.embeddings.word_embeddings.weight: copying a param with shape torch.Size([30349, 768]) from checkpoint, the shape in current model is torch.Size([119547, 768])."]}]},{"cell_type":"code","metadata":{"id":"vdiQmj7gQ9_A","colab_type":"code","outputId":"ec6f75a1-d694-4c41-ab5c-f5b56af69ba0","executionInfo":{"status":"ok","timestamp":1578138573436,"user_tz":-540,"elapsed":2245,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import time\n","time_i = time.time()\n","model_bert.load_state_dict(odic_weight_new)\n","print(time.time() - time_i)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["0.2991673946380615\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3gjQq7j_RMb8","colab_type":"text"},"source":["# classifier 레이어 합치기(진행중)"]},{"cell_type":"code","metadata":{"id":"CcSEJCmpPAwT","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=2,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMabV8keP3lg","colab_type":"text"},"source":["# class BERTClassifier(nn.Module):"]},{"cell_type":"code","metadata":{"id":"j5oHuvkjRPEE","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=2,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","\n","    def forward(self, token_ids, token_type_ids, attention_masks, labels):\n","        \n","        _, pooler = self.bert(input_ids = token_ids, \n","                              token_type_ids = token_type_ids, \n","                              attention_mask = attention_masks.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        loss = nn.CrossEntropyLoss(out, labels)\n","        return loss, self.classifier(out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4LrVnTQRU9j","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda:0\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82hAus2JRQoM","colab_type":"code","colab":{}},"source":["model = BERTClassifier(model_bert,  dr_rate=0.1).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBG4w_oLRhys","colab_type":"code","outputId":"eea0299f-500c-47e4-98a7-85730abb2939","executionInfo":{"status":"ok","timestamp":1578128987595,"user_tz":-540,"elapsed":15990,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["!nvidia-smi    # 1183MiB 차지    \n","# 200104(토) 17:51 1003Mb 차지/ Tesla P4"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Sat Jan  4 09:09:40 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    23W /  75W |   1003MiB /  7611MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"htIKk6FVR85R","colab_type":"text"},"source":["# 데이터 로드 및 전처리"]},{"cell_type":"code","metadata":{"id":"sbF4mDdNR-jd","colab_type":"code","outputId":"a0ecd388-0d77-45cb-a4b6-2927e25a2b0f","executionInfo":{"status":"ok","timestamp":1578138593634,"user_tz":-540,"elapsed":2162,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":263}},"source":["# 판다스로 훈련셋과 테스트셋 데이터 로드\n","import pandas as pd\n","path_base = '/content/drive/My Drive/금융문자/데이터/'\n","# train = pd.read_csv(path_base + \"sam60K_lvDist_0_82222_1_whole.csv\", encoding='utf-8')\n","train = pd.read_csv(path_base + \"sam32K_lvDist_0_85_1_2.csv\", encoding='utf-8')    # 200104(토) 17:51\n","print(train.shape)\n","train.head()"],"execution_count":50,"outputs":[{"output_type":"stream","text":["(32134, 7)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year_month</th>\n","      <th>text</th>\n","      <th>smishing</th>\n","      <th>prep</th>\n","      <th>len</th>\n","      <th>lvDist</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>303496</td>\n","      <td>2018-09</td>\n","      <td>ㅡ</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>147557</td>\n","      <td>2017-08</td>\n","      <td>XXXㅐㅐ</td>\n","      <td>0</td>\n","      <td></td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>257777</td>\n","      <td>2018-04</td>\n","      <td>XXX0415 14:22XXXXXXXX카드스마트XXX710000잔액4869889</td>\n","      <td>0</td>\n","      <td>04151422 카드스마트 710000잔액4869889</td>\n","      <td>31</td>\n","      <td>0.967742</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>84368</td>\n","      <td>2017-05</td>\n","      <td>XX-XXX-XXX0전화하셔서0번누르시면상담원연결됩니다.사용자계약번호확인해주세요</td>\n","      <td>0</td>\n","      <td>0전화하셔서0번누르시면상담원연결됩니다사용자계약번호확인해주세요</td>\n","      <td>34</td>\n","      <td>0.947368</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>81268</td>\n","      <td>2017-05</td>\n","      <td>XXX-XX-XXX-XXX (1XXX입니다.감사합니다XXX은행XXX올림</td>\n","      <td>0</td>\n","      <td>1 입니다감사합니다 은행 올림</td>\n","      <td>17</td>\n","      <td>0.911765</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id year_month  ... len    lvDist\n","0  303496    2018-09  ...   0  1.000000\n","1  147557    2017-08  ...   1  1.000000\n","2  257777    2018-04  ...  31  0.967742\n","3   84368    2017-05  ...  34  0.947368\n","4   81268    2017-05  ...  17  0.911765\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"7ZCKBOKISHwS","colab_type":"code","outputId":"c516b015-4805-44be-dff3-602de848ba42","executionInfo":{"status":"ok","timestamp":1578138593635,"user_tz":-540,"elapsed":1555,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import re\n","re_pat_space = re.compile('\\s{2,}')\n","re_pat_space.sub('g', '안녕  하세       요')\n","re_pat_words = re.compile('[^가-힣a-zA-Z0-9\\s]')\n","re_pat_words.findall(\"녕  하세       요\")\n","re_pat_XX_XXX = re.compile('X{2,}')\n","print(re_pat_XX_XXX.findall('XX고객님 XXX은행입니다. 안녕  하세       요X'))\n","re_pat_XX_XXX.sub('', 'XX고객님 XXX은행입니다. 안녕  하세       요X')"],"execution_count":51,"outputs":[{"output_type":"stream","text":["['XX', 'XXX']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'고객님 은행입니다. 안녕  하세       요X'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"F1RLyj6nSLfq","colab_type":"code","outputId":"684d1a1e-65c9-42c1-dc10-ac3aeaef5f2f","executionInfo":{"status":"ok","timestamp":1578128988072,"user_tz":-540,"elapsed":3990,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":605}},"source":["train['text'] = train['text'].apply(lambda x: re_pat_XX_XXX.sub(' ', re_pat_space.sub(' ', re_pat_words.sub('', x))))\n","train#['text']"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year_month</th>\n","      <th>text</th>\n","      <th>smishing</th>\n","      <th>prep</th>\n","      <th>len</th>\n","      <th>lvDist</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>303496</td>\n","      <td>2018-09</td>\n","      <td></td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>147557</td>\n","      <td>2017-08</td>\n","      <td></td>\n","      <td>0</td>\n","      <td></td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>257777</td>\n","      <td>2018-04</td>\n","      <td>0415 1422 카드스마트 710000잔액4869889</td>\n","      <td>0</td>\n","      <td>04151422 카드스마트 710000잔액4869889</td>\n","      <td>31</td>\n","      <td>0.967742</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>84368</td>\n","      <td>2017-05</td>\n","      <td>0전화하셔서0번누르시면상담원연결됩니다사용자계약번호확인해주세요</td>\n","      <td>0</td>\n","      <td>0전화하셔서0번누르시면상담원연결됩니다사용자계약번호확인해주세요</td>\n","      <td>34</td>\n","      <td>0.947368</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>81268</td>\n","      <td>2017-05</td>\n","      <td>1 입니다감사합니다 은행 올림</td>\n","      <td>0</td>\n","      <td>1 입니다감사합니다 은행 올림</td>\n","      <td>17</td>\n","      <td>0.911765</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>32129</th>\n","      <td>293353</td>\n","      <td>2018-08</td>\n","      <td>항상  하나은행을 이용해주셔서 감사합니다2018년 08월 신상품이 출시되여서 안내드...</td>\n","      <td>1</td>\n","      <td>항상 하나은행을이용해주셔서감사합니다2018년08월신상품이출시되여서안내드립니다낮은금리...</td>\n","      <td>316</td>\n","      <td>0.946203</td>\n","    </tr>\n","    <tr>\n","      <th>32130</th>\n","      <td>284216</td>\n","      <td>2018-07</td>\n","      <td>항상 이용해주셔서 감사합니다 2018년 정부지원 상품이 개편되어 안내드립니다 중고금...</td>\n","      <td>1</td>\n","      <td>항상이용해주셔서감사합니다2018년정부지원상품이개편되어안내드립니다중고금리대출을이용중이...</td>\n","      <td>493</td>\n","      <td>0.949290</td>\n","    </tr>\n","    <tr>\n","      <th>32131</th>\n","      <td>287940</td>\n","      <td>2018-07</td>\n","      <td>햇살론  대  안내광고  고객님께 빛이 되는  본상품은 서민금융으로정부에서 지원하고...</td>\n","      <td>1</td>\n","      <td>햇살론 대 안내광고 고객님께빛이되는 본상품은서민금융으로정부에서지원하고당사에서판매하는...</td>\n","      <td>480</td>\n","      <td>0.975000</td>\n","    </tr>\n","    <tr>\n","      <th>32132</th>\n","      <td>287290</td>\n","      <td>2018-07</td>\n","      <td>햇살론 에서 안내드립니다광고  고객님의 빛이 되어 드리는  당사상품의 자격기준과 심...</td>\n","      <td>1</td>\n","      <td>햇살론 에서안내드립니다광고 고객님의빛이되어드리는 당사상품의자격기준과심사기준이완화되어...</td>\n","      <td>591</td>\n","      <td>0.776650</td>\n","    </tr>\n","    <tr>\n","      <th>32133</th>\n","      <td>303566</td>\n","      <td>2018-09</td>\n","      <td>햇살론대출 지원금융센터  2018년 현재 정부지원 자금을 통해 하반기 채무통합대환상...</td>\n","      <td>1</td>\n","      <td>햇살론대출지원금융센터 2018년현재정부지원자금을통해하반기채무통합대환상품추가상품개편되...</td>\n","      <td>640</td>\n","      <td>0.934375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>32134 rows × 7 columns</p>\n","</div>"],"text/plain":["           id year_month  ...  len    lvDist\n","0      303496    2018-09  ...    0  1.000000\n","1      147557    2017-08  ...    1  1.000000\n","2      257777    2018-04  ...   31  0.967742\n","3       84368    2017-05  ...   34  0.947368\n","4       81268    2017-05  ...   17  0.911765\n","...       ...        ...  ...  ...       ...\n","32129  293353    2018-08  ...  316  0.946203\n","32130  284216    2018-07  ...  493  0.949290\n","32131  287940    2018-07  ...  480  0.975000\n","32132  287290    2018-07  ...  591  0.776650\n","32133  303566    2018-09  ...  640  0.934375\n","\n","[32134 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"42Sa75D7SPZq","colab_type":"code","outputId":"95f9a43e-c941-4690-b42d-2034348398e8","executionInfo":{"status":"ok","timestamp":1578128995839,"user_tz":-540,"elapsed":2242,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":488}},"source":["import matplotlib.pyplot as plt\n","train_rand = train.sample(frac=1, random_state=2020).reset_index()\n","print(train_rand['smishing'].head())\n","print('ratio of smishing:', train['smishing'].sum()/len(train['smishing']))\n","train['smishing'].hist()\n","plt.figure(figsize=(100,1))\n","plt.scatter(train_rand['smishing'].index.values, train_rand['smishing'].values, s=1)\n","plt.show()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0    1\n","1    1\n","2    0\n","3    0\n","4    0\n","Name: smishing, dtype: int64\n","ratio of smishing: 0.06693844526047177\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAFecAAABWCAYAAADrw+l5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 7200x72 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tIcTFNanSWBq","colab_type":"code","outputId":"e03c0e23-1b92-403f-c894-ac2fdff02827","executionInfo":{"status":"ok","timestamp":1578128997948,"user_tz":-540,"elapsed":658,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["# 리뷰 문장 추출\n","sentences = train_rand['text']\n","print(sentences[:3])\n","print(len(sentences))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["0    mms 발송광고 안녕하세요   여신영업부입니다장문의 문자를 드려 정말 죄송합니다그럼...\n","1    국민의 평생 금융파트너   입니다 광고     안녕하세요   영업부입니다 바쁘신 와...\n","2                      신년달력 고객님1226일브아피룸에서만교부합니다 은행응암 \n","Name: text, dtype: object\n","32134\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BJ9jZQudSV-9","colab_type":"code","outputId":"62c6c5ca-eb72-4e95-f323-a2fb7237c595","executionInfo":{"status":"ok","timestamp":1578129000103,"user_tz":-540,"elapsed":1117,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import re\n","def f_re_pat(x):\n","#     print('before :', x)\n","    re_pat_ending = re.compile('(니다)|(세요)|(시오)')\n","    idx = 0\n","    for i in range(10):\n","#         print(idx, x[idx:])\n","        tmp_search = re_pat_ending.search(x[idx:])\n","        if tmp_search!=None:\n","            idx = tmp_search.end() +idx\n","            x = x[:idx] + ' [SEP]' + x[idx:]\n","#             print('after  :', x)\n","            idx += 6    # len(' [SEP]')\n","        else:\n","#             print('None')\n","            break\n","    if x[-6:] != ' [SEP]':    # 마지막 문장을 if문으로 판단하여, [SEP]를 넣어준다.\n","        x = x + ' [SEP]'\n","    return x\n","f_re_pat(train['text'][2])\n","# f_re_pat('안녕하세요. 입니다.')"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' 0415 1422 카드스마트 710000잔액4869889 [SEP]'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"vDheONJ5SV8b","colab_type":"code","outputId":"6f5eab2e-32e6-445a-b88c-316e8627e6d3","executionInfo":{"status":"ok","timestamp":1578129008172,"user_tz":-540,"elapsed":1571,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":137}},"source":["import time\n","time_i = time.time()\n","sentences = sentences.apply(f_re_pat).values\n","print(sentences[:5])\n","print(time.time() - time_i)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["['mms 발송광고 안녕하세요 [SEP]   여신영업부입니다 [SEP]장문의 문자를 드려 정말 죄송합니다 [SEP]그럼에도 문자를 보내드리는것은한국자산관리공사 정부기관 자금지원으로 실행되는 국민행복 마이너스통장대출 안내차 연락 드렸습니다 [SEP] 상품 개요 마이너스 통장 형태의 저신용자 및 고금리 사용자 대출전환 지원 상품 대출한도 500만원 8000만원 까지 개인별 신용및 직급에따라 차이 있음 이율 최저 25 최고 85 자격 조건 사업자 사업자 등록증 소지자로써정상적인 사업 활동자급여소득자 3개월 이상 급여소득 확인자주부 고금리 사용자 및 신규 대출 신청자무직자 고금리 사용자 및 신규 대출 신청자 담당자와 별도의 상담 진행 절차 담당자 배정 및 접수 대출 한도 이율 산정 및 승인 신청자 거래지점 파악 및 진행 지점 배정 신청자 배정 지점으로 승인서 발송 신청자 서류 지참후 지점 방문 접수 및최종 대출금 수령 담당자는 상담 및 심사만 도움드리며 최종   고객님께서 직접 가까운 지점을 방문하셔야대출금 대출카드 수령이 가능하십니다 [SEP] 상담 가능 시간 오전 900오후 1800전국은행연합회등록번호 0406002 담당자 강  대리카카오톡  KANG83대표번호   장문의 내용을 읽어 주셔서 감사합니다 [SEP]언제든지 연락 주신  고객님을 위한최고의 서비스를 약속 드리겠습니다 [SEP] 무료거부  mms 발송 [SEP]'\n"," '국민의 평생 금융파트너   입니다 [SEP] 광고     안녕하세요 [SEP]   영업부입니다 [SEP] 바쁘신 와중에 각종 홍보전화와 문자를 받으셨을텐데 우선 장문의 문자를 드려 정말 죄송합니다 [SEP] 그럼에도 문자를 보내드리는것은 한국자산관리공사 정부기관 자금지원으로 실행되는  국민행복 마이너스통장으로대출 안내차 연락 드렸습니다 [SEP] 상품 개요 마이너스 통장 형태의 저신용자 및 고금리 사용자 대출 전환 지원 상품 대출한도 500만원 8000만원 까지 대출 한도는 개인신용등급에 따라 차등적용 됩니다 [SEP] 이율 최저 25 최고 85 자격 요건 사업자 사업자 등록증 소지자로써 정상적인 사업 활동자 급여소득자 3개월 이상 급여소득   주부 [SEP]'\n"," '신년달력 고객님1226일브아피룸에서만교부합니다 [SEP] 은행응암  [SEP]'\n"," '  고객님에일린의 뜰 중도금 대출 을 0130일까지 잔금대출로 전환을 하시든지 아니면 전액상환을 해야 되는바상환이 불가시에는 중도금 대출이자를 2017년11월 부터 본인이 부담해야 하므로  은행 보통예금통장을 개설하여만들어서 자동이체를 해야 연체이자 부담을 하지 않음을 알려 드립니다 [SEP]가까운  은행을 방문하여 보통예금 통장을 개설하여 자동이체 신청 부탁드립니다 [SEP]'\n"," '  고객님감사하는 마음은 불행을 막아주는마법의 열쇠라고 합니다 [SEP]작지만 소소한 행복일상의고마움으로남은 6월행복으로 가득 채우세요 [SEP] 은행 강서지점 팀장올림 [SEP]']\n","0.40509533882141113\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oymF0l90RrVF","colab_type":"code","outputId":"fd3f5494-4e44-44b7-ab5c-7c195430d792","executionInfo":{"status":"ok","timestamp":1578129021393,"user_tz":-540,"elapsed":4034,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["100%|██████████| 995526/995526 [00:01<00:00, 779572.69B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2SJmTo2YSV5i","colab_type":"code","outputId":"7bb5cf88-b978-42b1-a524-3e8628bc6f12","executionInfo":{"status":"ok","timestamp":1578129062835,"user_tz":-540,"elapsed":30766,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import time\n","time_i = time.time()\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])\n","print(time.time() - time_i)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["mms 발송광고 안녕하세요 [SEP]   여신영업부입니다 [SEP]장문의 문자를 드려 정말 죄송합니다 [SEP]그럼에도 문자를 보내드리는것은한국자산관리공사 정부기관 자금지원으로 실행되는 국민행복 마이너스통장대출 안내차 연락 드렸습니다 [SEP] 상품 개요 마이너스 통장 형태의 저신용자 및 고금리 사용자 대출전환 지원 상품 대출한도 500만원 8000만원 까지 개인별 신용및 직급에따라 차이 있음 이율 최저 25 최고 85 자격 조건 사업자 사업자 등록증 소지자로써정상적인 사업 활동자급여소득자 3개월 이상 급여소득 확인자주부 고금리 사용자 및 신규 대출 신청자무직자 고금리 사용자 및 신규 대출 신청자 담당자와 별도의 상담 진행 절차 담당자 배정 및 접수 대출 한도 이율 산정 및 승인 신청자 거래지점 파악 및 진행 지점 배정 신청자 배정 지점으로 승인서 발송 신청자 서류 지참후 지점 방문 접수 및최종 대출금 수령 담당자는 상담 및 심사만 도움드리며 최종   고객님께서 직접 가까운 지점을 방문하셔야대출금 대출카드 수령이 가능하십니다 [SEP] 상담 가능 시간 오전 900오후 1800전국은행연합회등록번호 0406002 담당자 강  대리카카오톡  KANG83대표번호   장문의 내용을 읽어 주셔서 감사합니다 [SEP]언제든지 연락 주신  고객님을 위한최고의 서비스를 약속 드리겠습니다 [SEP] 무료거부  mms 발송 [SEP]\n","['mm', '##s', '발', '##송', '##광', '##고', '안', '##녕', '##하', '##세', '##요', '[SEP]', '여', '##신', '##영', '##업', '##부', '##입', '##니다', '[SEP]', '장', '##문', '##의', '문', '##자를', '드', '##려', '정', '##말', '죄', '##송', '##합', '##니다', '[SEP]', '그', '##럼', '##에도', '문', '##자를', '보', '##내', '##드', '##리는', '##것', '##은', '##한', '##국', '##자', '##산', '##관', '##리', '##공', '##사', '정', '##부', '##기', '##관', '자', '##금', '##지', '##원으로', '실', '##행', '##되는', '국', '##민', '##행', '##복', '마', '##이', '##너', '##스', '##통', '##장', '##대', '##출', '안', '##내', '##차', '연', '##락', '드', '##렸', '##습', '##니다', '[SEP]', '상', '##품', '개', '##요', '마', '##이', '##너', '##스', '통', '##장', '형', '##태', '##의', '저', '##신', '##용', '##자', '및', '고', '##금', '##리', '사', '##용', '##자', '대', '##출', '##전', '##환', '지', '##원', '상', '##품', '대', '##출', '##한', '##도', '500', '##만', '##원', '8000', '##만', '##원', '까', '##지', '개', '##인', '##별', '신', '##용', '##및', '직', '##급', '##에', '##따', '##라', '차', '##이', '있', '##음', '이', '##율', '최', '##저', '25', '최고', '85', '자', '##격', '조', '##건', '사', '##업', '##자', '사', '##업', '##자', '등', '##록', '##증', '소', '##지', '##자로', '##써', '##정', '##상', '##적인', '사', '##업', '활', '##동', '##자', '##급', '##여', '##소', '##득', '##자', '3', '##개', '##월', '이상', '급', '##여', '##소', '##득', '확인', '##자', '##주', '##부', '고', '##금', '##리', '사', '##용', '##자', '및', '신', '##규', '대', '##출', '신', '##청', '##자', '##무', '##직', '##자', '고', '##금', '##리', '사', '##용', '##자', '및', '신', '##규', '대', '##출', '신', '##청', '##자', '담', '##당', '##자와', '별', '##도의', '상', '##담', '진', '##행', '절', '##차', '담', '##당', '##자', '배', '##정', '및', '접', '##수', '대', '##출', '한', '##도', '이', '##율', '산', '##정', '및', '승', '##인', '신', '##청', '##자', '거', '##래', '##지', '##점', '파', '##악', '및', '진', '##행', '지', '##점', '배', '##정', '신', '##청', '##자', '배', '##정', '지', '##점', '##으로', '승', '##인', '##서', '발', '##송', '신', '##청', '##자', '서', '##류', '지', '##참', '##후', '지', '##점', '방', '##문', '접', '##수', '및', '##최', '##종', '대', '##출', '##금', '수', '##령', '담', '##당', '##자는', '상', '##담', '및', '심', '##사', '##만', '도', '##움', '##드', '##리', '##며', '최', '##종', '고', '##객', '##님', '##께', '##서', '직접', '가', '##까', '##운', '지', '##점을', '방', '##문', '##하', '##셔', '##야', '##대', '##출', '##금', '대', '##출', '##카', '##드', '수', '##령', '##이', '가', '##능', '##하', '##십', '##니다', '[SEP]', '상', '##담', '가', '##능', '시', '##간', '오', '##전', '900', '##오', '##후', '1800', '##전', '##국', '##은', '##행', '##연', '##합', '##회', '##등', '##록', '##번', '##호', '040', '##60', '##02', '담', '##당', '##자', '강', '대', '##리', '##카', '##카', '##오', '##톡', 'KA', '##NG', '##83', '##대', '##표', '##번', '##호', '장', '##문', '##의', '내', '##용을', '읽', '##어', '주', '##셔', '##서', '감', '##사', '##합', '##니다', '[SEP]', '언', '##제', '##든', '##지', '연', '##락', '주', '##신', '고', '##객', '##님', '##을', '위한', '##최', '##고', '##의', '서', '##비스', '##를', '약', '##속', '드', '##리', '##겠', '##습', '##니다', '[SEP]', '무', '##료', '##거', '##부', 'mm', '##s', '발', '##송', '[SEP]']\n","29.098654747009277\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yhykAMBJSV2S","colab_type":"code","outputId":"868a1109-4cc7-4c5f-b4bb-fea472d4188c","executionInfo":{"status":"ok","timestamp":1578131517516,"user_tz":-540,"elapsed":1727,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# 라벨 추출\n","labels = train_rand['smishing'].values\n","print(labels)"],"execution_count":89,"outputs":[{"output_type":"stream","text":["[1 1 0 ... 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FmUNlyUrTCad","colab_type":"text"},"source":["## token2id, padding, attention masking"]},{"cell_type":"code","metadata":{"id":"BxUcSIsRSxPZ","colab_type":"code","outputId":"e9de5ffa-8ba3-433c-ebf2-ad4a81d019cc","executionInfo":{"status":"ok","timestamp":1578131553134,"user_tz":-540,"elapsed":5247,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import time\n","time_i = time.time()\n","# 입력 토큰의 최대 시퀀스 길이\n","\n","MAX_LEN = 512#192#128#512#128#512#191227(금)_01   #128191226(목)\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(time.time() - time_i)\n","input_ids[0]"],"execution_count":90,"outputs":[{"output_type":"stream","text":["3.665862560272217\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([ 10366,  10107,   9323, 119057, 118649,  11664,   9521, 118741,\n","        35506,  24982,  48549,    102,   9565,  25387,  30858,  26784,\n","        14646,  58303,  48345,    102,   9657,  25934,  10459,   9297,\n","        48959,   9113,  26737,   9670,  89523,   9687, 119057,  33188,\n","        48345,    102,   8924, 118866,  35979,   9297,  48959,   9356,\n","        31605,  15001,  26344, 118627,  10892,  11102,  20479,  13764,\n","        21386,  20595,  12692,  28000,  12945,   9670,  14646,  12310,\n","        20595,   9651,  40032,  12508,  78686,   9489,  25549,  24683,\n","         8909,  36553,  25549,  70915,   9246,  10739,  70162,  12605,\n","        43022,  13890,  14423,  52363,   9521,  31605,  23466,   9568,\n","       107693,   9113, 118881, 119081,  48345,    102,   9414,  52951,\n","         8857,  48549,   9246,  10739,  70162,  12605,   9879,  13890,\n","         9983,  83616,  10459,   9663,  25387,  24974,  13764,   9316,\n","         8888,  40032,  12692,   9405,  24974,  13764,   9069,  52363,\n","        16617,  51745,   9706,  14279,   9414,  52951,   9069,  52363,\n","        11102,  12092,  10757,  19105,  14279,  36962,  19105,  14279,\n","         8939,  12508,   8857,  12030,  61844,   9487,  24974, 118961,\n","         9707,  37568,  10530, 118825,  17342,   9730,  10739,   9647,\n","        32158,   9638, 119183,   9764,  48387,  10258,  83491,  12017,\n","         9651,  45465,   9678,  71439,   9405,  26784,  13764,   9405,\n","        26784,  13764,   9121,  31398, 119230,   9448,  12508,  57713,\n","        73131,  16605,  14871,  15387,   9405,  26784,   9996,  18778,\n","        13764,  37568,  29935,  22333, 118813,  13764,    124,  21789,\n","        38851,  66982,   8929,  29935,  22333, 118813,  84300,  13764,\n","        16323,  14646,   8888,  40032,  12692,   9405,  24974,  13764,\n","         9316,   9487,  69753,   9069,  52363,   9487,  40311,  13764,\n","        32537,  33077,  13764,   8888,  40032,  12692,   9405,  24974,\n","        13764,   9316,   9487,  69753,   9069,  52363,   9487,  40311,\n","        13764,   9064,  21928,  89389,   9353,  54469,   9414, 105462,\n","         9708,  25549,   9666,  23466,   9064,  21928,  13764,   9330,\n","        16605,   9316,   9669,  15891,   9069,  52363,   9954,  12092,\n","         9638, 119183,   9407,  16605,   9316,   9484,  12030,   9487,\n","        40311,  13764,   8863,  37388,  12508,  34907,   9901, 119110,\n","         9316,   9708,  25549,   9706,  34907,   9330,  16605,   9487,\n","        40311,  13764,   9330,  16605,   9706,  34907,  11467,   9484,\n","        12030,  12424,   9323, 119057,   9487,  40311,  13764,   9425,\n","        46520,   9706, 119251,  31531,   9706,  34907,   9328,  25934,\n","         9669,  15891,   9316, 119273,  22200,   9069,  52363,  40032,\n","         9460,  44220,   9064,  21928,  53639,   9414, 105462,   9316,\n","         9491,  12945,  19105,   9087, 119169,  15001,  12692,  21406,\n","         9764,  22200,   8888, 118617, 108578, 118683,  12424,  67288,\n","         8843, 118671,  21614,   9706,  67477,   9328,  25934,  35506,\n","       119049,  21711,  14423,  52363,  40032,   9069,  52363,  24206,\n","        15001,   9460,  44220,  10739,   8843,  74986,  35506, 119085,\n","        48345,    102,   9414, 105462,   8843,  74986,   9485,  18784,\n","         9580,  16617,  13545,  28188,  31531,  13648,  16617,  20479,\n","        10892,  25549,  25486,  33188,  14863, 101322,  31398,  35465,\n","        20309,  82738,  50924,  90426,   9064,  21928,  13764,   8853,\n","         9069,  12692,  24206,  24206,  28188, 119357,  85314,  34065,\n","        68073,  14423,  37824,  35465,  20309,   9657,  25934,  10459,\n","         8996,  72444,   9642,  12965,   9689, 119049,  12424,   8848,\n","        12945,  33188,  48345,    102,   9548,  17730,  90537,  12508,\n","         9568, 107693,   9689,  25387,   8888, 118617, 108578,  10622,\n","        28195, 119273,  11664,  10459,   9425, 104021,  11513,   9539,\n","        43962,   9113,  12692, 118632, 119081,  48345,    102,   9294,\n","        38688,  41521,  14646,  10366,  10107,   9323, 119057,    102,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0])"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"xTQo0DhYS_G6","colab_type":"code","outputId":"fbbde0e0-b9e8-4def-d786-19c679dcc60a","executionInfo":{"status":"ok","timestamp":1578131574127,"user_tz":-540,"elapsed":10992,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import time\n","time_i = time.time()\n","# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(len(attention_masks[0]))\n","print(attention_masks[0])\n","print(time.time() - time_i)"],"execution_count":91,"outputs":[{"output_type":"stream","text":["512\n","[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8.940956830978394\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LRPgIdmcS_CI","colab_type":"code","outputId":"59da0758-cc35-4304-b057-ae10c7b70ecc","executionInfo":{"status":"ok","timestamp":1578131587518,"user_tz":-540,"elapsed":1322,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 훈련셋과 검증셋으로 분리\n","nb_samples = 32*4\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids[:nb_samples],\n","                                                                                    labels[:nb_samples], \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks[:nb_samples], \n","                                                       input_ids[:nb_samples],\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":92,"outputs":[{"output_type":"stream","text":["tensor([  9414,  43962,  96279,  40032,  86080,   9521,  31605,   9632,  25549,\n","          9736,  14279,  12508,  34907,   9899,  13890,   9645,  48345,    102,\n","          9651,  50814,   9414,  43962,  96279,  40032,   9706,  37568,   9751,\n","         17196,  23466,   9414,  43962,  12030,  14646,   8935,  14040, 118639,\n","         10739,  39245,  18392,   9580,  31531,  10530,   8996,  34907,  13374,\n","          9358,  12030,   9706,  37712,  24747,  10892,   9460,  44220,  53529,\n","          8982, 118920,  12508,   9706,  37712,  24747,  10892,   9946,  14871,\n","         43962,  12030,  82875,  11261,   9576,  18622,  41693,  58303,  48345,\n","           102,   9678,  43962,  11102,   9485,  18392,  31605,  10530,   9425,\n","         46520,   9706, 119251,  13374,   9460,  44220,   9318, 118853,  48345,\n","           102,  73142,   9554,  32537,  14423,  29935,  40032,  11664,  11018,\n","          9414,  43962,  12030,   9665,  73295,   8996,  34907,  35506,  14040,\n","         14867,   9744,  44130,  74986,   9957,  48345,    102,   9069,  12692,\n","         12030,   8843,  74986,   9297,  10459,  12945,  50632,  10739,   9647,\n","        119185,  14040,  14867,   9899,  85903,  14153,  11261,   9665,  18227,\n","          9365, 119335,  33188,  48345,    102,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.])\n","tensor([  9680,  31720,  12453,   8848,  12945,  11102,   8888, 118617, 108578,\n","         12965, 118760, 119031,  10247,  37712,  47869,   9706,  16439, 118610,\n","         48345,    102,   9246, 105197,  10622,  38378,   9356,  61250,  11882,\n","          8931,  80795, 119192,  80046,   9685, 119185, 101440,  19105,   9414,\n","         65649,  10530,  52579,  11018,   9519, 119072, 119169,  12092,   8987,\n","         11018, 118817,  33188,  48345,    102,   9056,  11287,  82823,  10233,\n","         37712,  12310,   9415,  89851,  15303,  67324,  11513,   9551, 119089,\n","         34951,  10622,  15891,  84177,   8932, 119024,   8843, 118813,  35506,\n","         14040,  12310,   9318, 118853,  48345,    102,   9663,   8888, 118617,\n","        108578, 118683,   8932, 119024,  10622,   9113,  12692,  71689,   9022,\n","         28143,  12453,   9568,  23811,  35506, 118632, 119081,  48345,    102,\n","          9689,  89523,  15303,   8985,  49212,  11287,    100,    102,    100,\n","          9703, 118626,  11664,  25549,  70915,  11102,   9485,  18784, 118800,\n","        119085,  14040,  28188,    102,   9565,  38631,  12508,  34907,   9689,\n","         41521,  37388,  11664, 118617, 100420,   9730,  13890,   9706,  34907,\n","         13890,   9583,  67527,    102,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bcD_-YPNS--l","colab_type":"code","outputId":"08263929-88b5-4db8-e6a6-fa6b8292e5a4","executionInfo":{"status":"ok","timestamp":1578135809576,"user_tz":-540,"elapsed":1699,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":150}},"source":["print(train_inputs[0].shape)\n","print(train_labels[0].shape)\n","print('len:', len(train_labels[0].shape))\n","print(train_masks[0].shape)\n","print(validation_inputs[0].shape)\n","print(validation_labels[0].shape)\n","print('len:', len(validation_labels[0].shape))\n","print(validation_masks[0].shape)"],"execution_count":105,"outputs":[{"output_type":"stream","text":["torch.Size([512])\n","torch.Size([])\n","len: 0\n","torch.Size([512])\n","torch.Size([512])\n","torch.Size([])\n","len: 0\n","torch.Size([512])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pUAHim2uS-6K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"a1ff2d32-270a-4bed-a6dd-c2d64bbca9b0","executionInfo":{"status":"error","timestamp":1578138521114,"user_tz":-540,"elapsed":1678,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["# 배치 사이즈\n","batch_size = 12#32#8#16#32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":44,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-a594408cb6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 학습시 배치 사이즈 만큼 데이터를 가져옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"x1E8R3EgTOG9","colab_type":"text"},"source":["# 모델 생성 및 하이퍼파라미터 설정"]},{"cell_type":"code","metadata":{"id":"_d0jBUkgTrj_","colab_type":"code","colab":{}},"source":["## Setting parameters\n","max_len = 512#128\n","batch_size = 12#32\n","warmup_ratio = 0\n","num_epochs = 4\n","max_grad_norm = 1\n","log_interval = 10\n","learning_rate =  2e-5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBNYK9tYTcAm","colab_type":"code","colab":{}},"source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8) # eps 0으로 나누는 것을 방지하기 위한 epsilon 값\n","loss_fn = nn.CrossEntropyLoss()\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","# scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps=warmup_step, t_total=t_total) 원래이거였는데 바꿔줌\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LeXhdbhTkoE","colab_type":"text"},"source":["## 훈련 코드"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S0-p6pPVXCRe","colab":{}},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FJXISnJzCdLM","colab":{}},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"muU2kS2GCh4y","outputId":"8bd54ea1-86c9-4ee1-d776-5692ea0af43e","executionInfo":{"status":"ok","timestamp":1578138485039,"user_tz":-540,"elapsed":2556,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f1f6c2772b0>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"7qfOgKBQppwF","colab_type":"code","outputId":"d1c9eafd-38a5-45c0-ce1f-6f99672323d8","executionInfo":{"status":"error","timestamp":1578138488919,"user_tz":-540,"elapsed":2059,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["print(train_dataloader.batch_size)\n","print(train_dataloader.dataset.tensors[2][-2].shape)\n","print(len(train_dataloader.dataset.tensors))"],"execution_count":41,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-6df26a9f7808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"HDAbTQDUOu5b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"a8e85b0d-0184-42dc-fb57-09ad97cde8a2","executionInfo":{"status":"error","timestamp":1578138489421,"user_tz":-540,"elapsed":1120,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 학습률을 조금씩 감소시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":42,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-791577ae9263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 총 훈련 스텝 : 배치반복 횟수 * 에폭\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 학습률을 조금씩 감소시키는 스케줄러 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"3lZoC3fyrocZ","colab_type":"code","colab":{}},"source":["path_base = '/content/drive/My Drive/금융문자/모델링/models_from_hugging face/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"b59e2b44-cbf6-4f95-98c2-ac82d715e6d4","id":"AS11UBkNppwS","executionInfo":{"status":"error","timestamp":1578138501915,"user_tz":-540,"elapsed":2081,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":278}},"source":["# torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","    #@ trainset 정확도 초기화\n","    train_accuracy = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        None,  # token_type_ids=\n","                        b_input_mask,  # attention_mask=\n","                        b_labels) # labels=\n","        # 경과 정보 표시\n","#         if step % 10 == 0 and not step == 0:\n","        nb_acc_calc = 0\n","        if step in [10, 400, 800]:\n","            model.eval()\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","            # 출력 로짓과 라벨을 비교하여 정확도 계산\n","            time_acc = time.time()\n","            tmp_train_accuracy = flat_accuracy(outputs[1].detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n","            train_accuracy += tmp_train_accuracy\n","            nb_acc_calc += 1\n","            print(' Train acc : ', tmp_train_accuracy/nb_acc_calc)\n","            print(format_time(time.time() - time_acc))\n","\n","        model.train()\n","        # 로스 구함\n","        loss = outputs[0]\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","#         torch.save(model, 'weight_191228_samples30000_maxlen128_batchsize32')# + str(epoch_i) + '_' + str(step))\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","    \n","    \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            None, # token_type_ids=\n","                            b_input_mask) # attention_mask=\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","    #@ 모델 저장\n","    torch.save(model, path_base + 'weight_200102_samp32K(smsh2k)repSEP_X512_bs16' + str(epoch_i) + '_' + str(step))\n","    print('model weight has been saved.')\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","# 200102(목) ?23:35~ 시작(ep당 11분 소요예상, 23:46, 7sec/10batch)\n","# 23:36 1ep 완료 \n","# 23:45 1.16ep 도중 중단 (9/56 + 1)\n","\n","\n","# 200103(금) ?01:31~ 시작(ep당 11분 소요예상, 23:46, 7sec/10batch)\n","# 23:36 1ep 완료\n","# 03:30 4ep 완료"],"execution_count":43,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-bb69d2fdd961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 데이터로더에서 배치만큼 반복하여 가져옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# 배치를 GPU에 넣음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"9CPo5kbV_lFR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"6de1ef45-e0a8-4d4a-bf8a-46aa43c506f5","executionInfo":{"status":"error","timestamp":1578131939568,"user_tz":-540,"elapsed":1010,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["train_dataloader[0]"],"execution_count":99,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-99-dacfcf1893f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object does not support indexing"]}]},{"cell_type":"markdown","metadata":{"id":"WEf8exUwZHjr","colab_type":"text"},"source":["# SKTbert 방식 데이터 로더"]},{"cell_type":"code","metadata":{"id":"IUSmm6heZJ9H","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-HPDKjgRkNVG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"outputId":"05ba6f56-785b-414b-fac2-c040732dd51d","executionInfo":{"status":"error","timestamp":1578138694306,"user_tz":-540,"elapsed":1223,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()"],"execution_count":52,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-f9761b30907a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_grouped_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'optimizer_grouped_parameters' is not defined"]}]},{"cell_type":"code","metadata":{"id":"2CZQCmBNkNVS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"outputId":"57d60f43-1f3c-498a-87dc-6b6cb5f5752c","executionInfo":{"status":"error","timestamp":1578138694307,"user_tz":-540,"elapsed":683,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)"],"execution_count":53,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-c6b68efc3e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwarmup_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_total\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwarmup_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"9OBotYwukNVe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"13d3f650-a1cd-4e9c-9db8-07d940290dfc","executionInfo":{"status":"error","timestamp":1578138696271,"user_tz":-540,"elapsed":2060,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}}},"source":["scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=t_total)"],"execution_count":54,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-92ae8f705c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWarmupLinearSchedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'WarmupLinearSchedule' is not defined"]}]},{"cell_type":"code","metadata":{"id":"coycEMe1kNVz","colab_type":"code","colab":{}},"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9V16qMd9ZKEK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxC8zX2lTZNo","colab_type":"code","outputId":"dc72a628-2019-4713-cc87-c4cc47ee995c","executionInfo":{"status":"error","timestamp":1578137078007,"user_tz":-540,"elapsed":1703,"user":{"displayName":"천성욱","photoUrl":"","userId":"08465277341498786357"}},"colab":{"base_uri":"https://localhost:8080/","height":277,"referenced_widgets":["b6e854515fe84a2ba0fa9e90e4861a3b","1fedb5ef7c6d46c8aace89ee3693e9be","b7807ed31aa54fd49fb7e51af75cf8c4","0c4ab22c2a544303a419f7b47a9d82e0","be597f2fc357461588cba80985b72ee4","cc79126fd444415197f4326ed04c593b","40cd1a185f2e43bd832a8bb7f8aca199","52dcb9b6c83a4668934f3dbc44e285e6"]}},"source":["from tqdm import tqdm, tqdm_notebook\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length,  label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        # segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"execution_count":128,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6e854515fe84a2ba0fa9e90e4861a3b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-128-515d1d252305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# segment_ids = segment_ids.long().to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mvalid_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"id":"gn3K0BThYYK1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}